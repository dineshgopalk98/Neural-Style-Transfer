{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_name = \"face1.jpg\"\n",
    "style_image_name = \"scr.jpg\"\n",
    "# Names for saved output images:\n",
    "save_image_as = \"created_image.jpg\"\n",
    "color_preserved_image = \"color_preserved_image.jpg\"\n",
    "\n",
    "random_start_image = False\n",
    "\n",
    "iterations = 300\n",
    "learning_rate = 0.1\n",
    "# number of iterations between showing losses:\n",
    "show_loss_rate = 10\n",
    "\n",
    "Adam_on = False\n",
    "adam_lr=0.005\n",
    "adam_wd=0.1\n",
    "\n",
    "#content-loss weight:\n",
    "alpha = 1  \n",
    "#style-loss weight:\n",
    "beta = 1000 \n",
    "\n",
    "# layers that we want output from the vgg19 network\n",
    "# The layer weights are relative and will be normalized to sum to 1\n",
    "content_layers = [21] # [21] -To remember the standard values we used when we experiment\n",
    "content_layers_weight = np.array([1]) #np.array([1])\n",
    "style_layers =  [0,5,10,19,28]    # [0,5,10,19,28]\n",
    "style_layers_weight = np.array([1,1,1,1,1]) #np.array([1,1,1,1,1])\n",
    "\n",
    "vgg_image_size = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.Resize(vgg_image_size),transforms.ToTensor()])\n",
    "                                        \n",
    "                                          \n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return tensor_transform(image).to(device)\n",
    "\n",
    "\n",
    "content_image = load_image(content_image_name)\n",
    "content_image_size = Image.open(content_image_name).size\n",
    "content_image_size = (content_image_size[1],content_image_size[0])\n",
    "\n",
    "\n",
    "style_image = load_image(style_image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_model = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "print(vgg19_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_weight = content_layers_weight/np.sum(content_layers_weight)\n",
    "style_layers_weight = style_layers_weight/np.sum(style_layers_weight)\n",
    "\n",
    "output_layers = content_layers + style_layers\n",
    "output_layers = list(set(output_layers))\n",
    "output_layers.sort()\n",
    "\n",
    "def extract_features(module, x, y):\n",
    "    features.append(y)\n",
    "    return\n",
    "\n",
    "for i, layer in enumerate(vgg19_model):\n",
    "    \n",
    "    if i in output_layers:\n",
    "        vgg19_model[i].register_forward_hook(extract_features)\n",
    "    \n",
    "    # \"A Neural Algorithm of Artistic Style\" by Gatys et Al. recommends using average pooling layers instead of max pooling.\n",
    "    if isinstance(layer, nn.MaxPool2d):\n",
    "        vgg19_model[i] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
    "\n",
    "for parameter in vgg19_model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "print(vgg19_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_content_features(features,output_layers,content_layers,style_layers):\n",
    "    content_features = []\n",
    "    style_features = []\n",
    "    for i, layer in enumerate(output_layers):\n",
    "        if layer in content_layers:\n",
    "            content_features.append(features[i])\n",
    "        if layer in style_layers:\n",
    "            style_features.append(features[i])\n",
    "    return content_features, style_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "vgg19_model(content_image.unsqueeze(0))\n",
    "\n",
    "content_target = style_content_features(features,output_layers,content_layers,style_layers)[0]\n",
    "\n",
    "content_target = [c.detach() for c in content_target]\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \n",
    "    b, c, h, w = x.size()\n",
    "    V = x.view(b,c,h*w)\n",
    "    gram_mat = torch.bmm(V, V.transpose(1,2))/(h*w)\n",
    "    return gram_mat\n",
    "\n",
    "features = []\n",
    "\n",
    "vgg19_model(style_image.unsqueeze(0))\n",
    "style_target = style_content_features(features,output_layers,content_layers,style_layers)[1]\n",
    "style_target = [s.detach() for s in style_target]\n",
    "gram_target =[gram_matrix(i).detach() for i in style_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_start_image:\n",
    "    image = torch.randn(3,vgg_image_size[0],vgg_image_size[1]).cuda().requires_grad_()\n",
    "else:\n",
    "    image = content_image.clone().requires_grad_()\n",
    "\n",
    "if Adam_on:\n",
    "    optimizer = Adam([image],lr=adam_lr,weight_decay=adam_wd)\n",
    "else:\n",
    "    optimizer = torch.optim.LBFGS([image],lr=learning_rate)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_image(image):\n",
    "    restore_image = transforms.Compose([transforms.Lambda(lambda x: x.clamp(0,1)),transforms.Resize(content_image_size)])\n",
    "    to_image = transforms.ToPILImage()\n",
    "    restored_picture = restore_image(image.squeeze().cpu().detach())\n",
    "    return np.array(to_image(restored_picture))\n",
    "\n",
    "def save_image(image_name,image):\n",
    "    plt.imsave(image_name,restore_image(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss_memory = []\n",
    "style_loss_memory = []\n",
    "    \n",
    "for i in range(iterations):\n",
    "    features = []\n",
    "    def closure(features=features,i=i):\n",
    "        optimizer.zero_grad()\n",
    "        vgg19_model(image.unsqueeze(0))\n",
    "        features = features[-1*len(output_layers):]\n",
    "        content_features, style_features = style_content_features(features,output_layers,content_layers,style_layers)\n",
    "        gram_styles = [gram_matrix(i) for i in style_features]\n",
    "        content_loss = 0\n",
    "        for i in range(len(content_features)):\n",
    "            content_loss += alpha * content_layers_weight[i] * mse_loss(content_features[i], content_target[i])/2\n",
    "        style_loss = 0\n",
    "        for i in range(len(gram_styles)):\n",
    "            style_loss += beta * style_layers_weight[i] * mse_loss(gram_styles[i],gram_target[i])/4\n",
    "        total_loss = content_loss + style_loss\n",
    "        content_loss_memory.append(content_loss)\n",
    "        style_loss_memory.append(style_loss)\n",
    "\n",
    "        total_loss.backward()\n",
    "    \n",
    "        return total_loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    \n",
    "    if (i % show_loss_rate == 0):\n",
    "        print(f\"content loss:{content_loss_memory[-1]:.6f}, style loss:{style_loss_memory[-1]:.6f}\")    \n",
    "        save_image(\"SoFar.jpg\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(save_image_as,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def restore_color(color_source, color_destination):\n",
    "    \n",
    "    c_source = np.moveaxis(color_source,0,2)\n",
    "    c_destination = np.moveaxis(color_destination,0,2)\n",
    "    \n",
    "    # extract luminance of destination:\n",
    "    gray_destination = cv2.cvtColor(c_destination, cv2.COLOR_BGR2GRAY) \n",
    "    # convert source from BGR to YIQ-YCbCr:\n",
    "    yiq_source = cv2.cvtColor(c_source, cv2.COLOR_BGR2YCrCb)           \n",
    "    # combining destinations luminance and sources IQ-CbCr:\n",
    "    yiq_source[...,0] = gray_destination                               \n",
    "    # converting new image from YIQ back to BGR:\n",
    "    return cv2.cvtColor(yiq_source, cv2.COLOR_YCrCb2BGR)               \n",
    "\n",
    "color_image = restore_image(image)\n",
    "color_image = load_image(save_image_as)\n",
    "color_image_np = color_image.cpu().numpy()\n",
    "content_image_np = content_image.cpu().numpy()\n",
    "restored_image = torch.tensor(np.moveaxis(restore_color(content_image_np,color_image_np),2,0))\n",
    "save_image(color_preserved_image,restored_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
